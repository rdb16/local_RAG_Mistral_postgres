
### installer sur votre ordinateur le serveur Ollama
    ## télécharger le LLM de mistral de manière à ce que les requêtes finales restent en interne
    ## Lancer le server Ollama avec mistral

### à la racine du projet créer un dossier config

### Remplir le fichier de configuration config.json et l'enregistrer dans le dossier config
{
  "IMAC_CONNECTION_STRING": "",
  "DATA_PATH": "./data-xxx",
  "DB_HOST": "",
  "DB_PORT": 5432,
  "DB_USER": "",
  "DB_PASS": "",
  "DB_NAME": "postgres"
}

### la base vectorielle sera crée sur un serveur Postgresql >= 14
    ## le programme se charge de créer la base si elle n'existe pas
    ## et de rajouter l'extension vector
    ## enfin de générer une table embeddings avec les champs définis ci-après

### populate.py:
 ## liste les fichiers trouvés dans le dossier data spécifié dans le fichier de conf
 ## liste les pdf déjà importés dans la base
 ## calcule la liste restreinte des nouveaux pdf à importer
 ## calcul l'embedding de ces fichiers et calcule l'index et génère les champs pour l'import
    # le nom du pdf
    # le type de document :  compte-rendu, notice, ordonnances, ...( liste à fournir)
    # l'index ids = numéro de page du pdf "." numéro du chunk de cette page ( utile pour citer les sources dans le résultat)
    # le texte du chunk
    # le nb de tokens du chunk
    # enfin le vecteur ( la vectorisation est faite avec un titan embed xx, qui retourne un vecteur de dim )
 ## Importe les embeddings de chaque chunk dans la base

 Notons que nous utilisons, pour une meilleure qualité de la recherche vectorielle,
 le moteur d'embeddings fournit par AWS et qui est payant.
 Il faut donc avoir à la racine de son Home le dossier .aws avec profil et credentials.
 Ne pas oublier de demander l'accès dans bedrock à ce modèle de fondation.
 Pour la recherche vectorielle, il est nécessaire de générer le vecteur de la question avec le même moteur
 d'embeddings que celui qui a utilisé pour nourrir la base.
 Le modèle de fondation d'embeddings utilisé par défaut par langchain_community.embeddings.bedrock
 dans la région us-east-1 est le modèle "Amazon Titan Embeddings G1 - Text", dont l'identifiant de base
            est amazon.titan-embed-text-v1.
 Ce modèle est conçu pour des tâches telles que la récupération de texte, la similarité sémantique
 et le clustering, avec une longueur maximale de texte d'entrée de 8 000 tokens
 et une longueur maximale de vecteur de sortie de 1 536 dimensions.

